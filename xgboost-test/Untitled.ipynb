{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DatasetFormat, DefaultModelMonitor\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::647453829825:role/service-role/AmazonSageMaker-ExecutionRole-20200729T003498'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess\n",
    "sm\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = sess.client('sts', region_name=sess.region_name).get_caller_identity()[\"Account\"]\n",
    "bucket = 'sagemaker-studio-{}-{}'.format(sess.region_name, account_id)\n",
    "prefix = 'HF_Prop_cal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647453829825\n",
      "sagemaker-studio-eu-west-1-647453829825\n",
      "HF_Prop_cal\n"
     ]
    }
   ],
   "source": [
    "print(account_id)\n",
    "print(bucket)\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-studio-i8kf84zy9f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker.amazon.amazon_estimator:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "#docker_image_name = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='0.90-2')\n",
    "docker_image_name = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "s3_marker201 = sagemaker.s3_input(s3_data='s3://{}/{}/{}'.format(bucket, prefix,'201.csv'), content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3.download_file('sagemaker-studio-i8kf84zy9f', 'HF_Prop_cal/201.csv', '_201.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate s3 resource\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# select bucket\n",
    "my_bucket = s3.Bucket('sagemaker-studio-i8kf84zy9f')\n",
    "def downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucketName) \n",
    "    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n",
    "        if (obj.key == remoteDirectoryName + \"/\"):\n",
    "            continue\n",
    "        if not os.path.exists(os.path.dirname(obj.key)):\n",
    "            os.makedirs(os.path.dirname(obj.key))\n",
    "        bucket.download_file(obj.key, obj.key)\n",
    "        \n",
    "downloadDirectoryFroms3('sagemaker-studio-i8kf84zy9f', 'HF_Prop_cal')\n",
    "downloadDirectoryFroms3('sagemaker-studio-i8kf84zy9f', 'oil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load oil markers...\n",
      "Load oil data from:  ['oil/oil-0.csv', 'oil/oil-0(1).csv']\n",
      "Total 454904 oil samples was loaded\n",
      "Size of oil dataset was reduced to 200000 samples\n",
      "Start train model for oil classification...\n",
      "Finish train model for oil classification. Filtered data frame size:  (67147, 9)\n",
      "['HF_Prop_cal/201.csv' 'HF_Prop_cal/202.csv' 'HF_Prop_cal/203.csv'\n",
      " 'HF_Prop_cal/205.csv' 'HF_Prop_cal/206.csv' 'HF_Prop_cal/209.csv'\n",
      " 'HF_Prop_cal/210.csv' 'HF_Prop_cal/212.csv' 'HF_Prop_cal/213.csv'\n",
      " 'HF_Prop_cal/214.csv']\n",
      "From file  HF_Prop_cal/201.csv  loaded dataframe of size:  (5115, 29)\n",
      "After filtering marker data frame size:  (4797, 9)\n",
      "From file  HF_Prop_cal/202.csv  loaded dataframe of size:  (2336, 29)\n",
      "After filtering marker data frame size:  (2177, 9)\n",
      "From file  HF_Prop_cal/203.csv  loaded dataframe of size:  (9533, 29)\n",
      "After filtering marker data frame size:  (8905, 9)\n",
      "From file  HF_Prop_cal/205.csv  loaded dataframe of size:  (2750, 29)\n",
      "After filtering marker data frame size:  (2612, 9)\n",
      "From file  HF_Prop_cal/206.csv  loaded dataframe of size:  (3122, 29)\n",
      "After filtering marker data frame size:  (2965, 9)\n",
      "From file  HF_Prop_cal/209.csv  loaded dataframe of size:  (3458, 29)\n",
      "After filtering marker data frame size:  (3285, 9)\n",
      "From file  HF_Prop_cal/210.csv  loaded dataframe of size:  (1516, 29)\n",
      "After filtering marker data frame size:  (1440, 9)\n",
      "From file  HF_Prop_cal/212.csv  loaded dataframe of size:  (3527, 29)\n",
      "After filtering marker data frame size:  (3349, 9)\n",
      "From file  HF_Prop_cal/213.csv  loaded dataframe of size:  (9361, 29)\n",
      "After filtering marker data frame size:  (8888, 9)\n",
      "From file  HF_Prop_cal/214.csv  loaded dataframe of size:  (1603, 29)\n",
      "After filtering marker data frame size:  (1522, 9)\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload # python 2.7 does not require this\n",
    "\n",
    "from geo_config import oil_files_list\n",
    "oil_files_list = [\"oil/oil-0.csv\",\n",
    "                  \"oil/oil-0(1).csv\"]\n",
    "import main_forest\n",
    "reload(main_forest)\n",
    "from main_forest import create_trainig_set\n",
    "oil_files_list\n",
    "X, y = create_trainig_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_T = np.transpose([y])\n",
    "data1 = np.empty((len(y), 0), dtype=np.object)\n",
    "data1 = np.append(data1, y.reshape(-1, 1), axis=1) \n",
    "data1 = np.append(data1, X, axis=1) \n",
    "#data = np.concatenate((y_T, X), axis=1)\n",
    "data = data1\n",
    "np.random.shuffle(data)\n",
    "#print(data)\n",
    "train_data, validation_data, test_data = np.split(data, [int(0.7 * len(data)), int(0.9 * len(data))])\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "train_data, validation_data, test_data = np.split(df.sample(frac=1, random_state=1729), [int(0.7 * len(df)), int(0.9 * len(df))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "test_data.to_csv('test.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "# np.savetxt(\"train.csv\", train_data, delimiter=\",\")\n",
    "# np.savetxt(\"validation.csv\", validation_data, delimiter=\",\")\n",
    "# np.savetxt(\"test.csv\", test_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 6.5792895915075595, 6.356858635796325, 5.834568198866494,\n",
       "        2.45178643552429, 2.456366033129043, 4.112102054770891,\n",
       "        5.736058497640671, 4.388190041599854, 4.164709663539879],\n",
       "       [10, 3.119585774961784, 2.816241299991783, 2.250420002308894,\n",
       "        1.863322860120456, 1.9637878273455553, 4.103393169973528,\n",
       "        3.3604040547299387, 2.0492180226701815, 1.5440680443502757],\n",
       "       [5, 6.225109667203757, 6.587406403231895, 5.981935882545361,\n",
       "        3.074450718954591, 3.058426024457005, 5.907307074579468,\n",
       "        5.532584958366993, 6.571627786992635, 6.258422955557263],\n",
       "       [5, 6.407728689877719, 6.792883354657924, 6.18653086867832,\n",
       "        3.0824263008607717, 3.064832219738574, 6.089202964360833,\n",
       "        5.82727038224852, 6.724262676681238, 6.411457404368348],\n",
       "       [10, 2.531478917042255, 3.036628895362161, 0.3010299956639812,\n",
       "        2.05307844348342, 2.1702617153949575, 3.6009728956867484,\n",
       "        2.8419848045901137, 1.3222192947339193, 1.8129133566428555]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix=''\n",
    "s3_input_train = boto3.Session().resource('s3').Bucket('sagemaker-studio-eu-west-1-647453829825').Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "s3_input_validation = boto3.Session().resource('s3').Bucket('sagemaker-studio-eu-west-1-647453829825').Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "markers_experiment = Experiment.create(experiment_name=\"geosplit-classification-{}\".format(create_date), \n",
    "                                              description=\"Using xgboost to predict markers\", \n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = sagemaker.Session()\n",
    "prefix = ''\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/train'.format('sagemaker-studio-eu-west-1-647453829825'), content_type='csv') \n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/validation/'.format('sagemaker-studio-eu-west-1-647453829825'), content_type='csv')\n",
    "\n",
    "# hyperparams = {\"max_depth\":5,\n",
    "#                \"subsample\":0.8,\n",
    "#                \"num_round\":600,\n",
    "#                \"eta\":0.2,\n",
    "#                \"gamma\":4,\n",
    "#                \"min_child_weight\":6,\n",
    "#                \"silent\":0,\n",
    "#                \"objective\":'logistic'}\n",
    "hyperparams = {\"max_depth\":5,\n",
    "               \"subsample\":0.8,\n",
    "               \"num_round\":25,\n",
    "               \"eta\":0.2,\n",
    "               \"gamma\":4,\n",
    "               \"min_child_weight\":6,\n",
    "               \"silent\":0,\n",
    "               \"num_class\": 11,\n",
    "               \"objective\": \"multi:softprob\"}#,\n",
    "               #\"eval_metric\": \"error\"}\n",
    "trial = Trial.create(trial_name=\"algorithm-mode-trial-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
    "                     experiment_name=markers_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "xgb = sagemaker.estimator.Estimator(image_name=docker_image_name,\n",
    "                                    role=role,\n",
    "                                    hyperparameters=hyperparams,\n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path='s3://{}/output'.format(bucket),\n",
    "                                    base_job_name=\"demo-xgboost\",\n",
    "                                    sagemaker_session=sess)\n",
    "#xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation},\n",
    "       experiment_config={\n",
    "            \"ExperimentName\": markers_experiment.experiment_name, \n",
    "            \"TrialName\": trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndpointName = demo-xgboost-geosplit-2020-08-07-19-31-39\n"
     ]
    }
   ],
   "source": [
    "data_capture_prefix = '{}/datacapture'.format('xgboost-geosplit')\n",
    "endpoint_name = \"demo-xgboost-geosplit-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName = {}\".format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: demo-xgboost-geosplit-2020-07-31-15-35-22\n"
     ]
    }
   ],
   "source": [
    "sess.delete_endpoint(xgb_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "INFO:sagemaker:Creating model with name: demo-xgboost-2020-08-07-19-21-40-607\n",
      "INFO:sagemaker:Creating endpoint with name demo-xgboost-geosplit-2020-08-07-19-31-39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1, \n",
    "                           instance_type='ml.m4.xlarge',\n",
    "                           endpoint_name=endpoint_name,\n",
    "                           data_capture_config=DataCaptureConfig(enable_capture=True,\n",
    "                                                                 sampling_percentage=100,\n",
    "                                                                 destination_s3_uri='s3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "                                                                )\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=1):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.to_numpy()[:1, 1:])\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.08181879785487 4.411888952395849 3.754501229386917 1.9344984512435677\n",
      " 2.187520720836463 5.995385952115371 6.413065953467973 2.6757783416740852\n",
      " 2.6848453616444123]\n",
      "pred =  b'[0.0018283948302268982, 0.9893065094947815, 0.000868268427439034, 0.0008486137958243489, 0.000848134804982692, 0.0008490047766827047, 0.0008541177958250046, 0.001368816359899938, 0.0008498586248606443, 0.0008519701077602804, 0.001526355859823525],[0.0009593224385753274, 0.0009361457778140903, 0.0009492176468484104, 0.0009157682070508599, 0.0009152504499070346, 0.000996708287857473, 0.0009217077749781311, 0.0009576377342455089, 0.9905390739440918, 0.0009193900623358786, 0.0009897802956402302],[0.0008750646375119686, 0.0008761147037148476, 0.0008825373952277005, 0.0008625597110949457, 0.0008620720473118126, 0.00086295633809641, 0.0008681542240083218, 0.0008730169502086937, 0.0008638245635665953, 0.0008659712038934231, 0.99130779504776],[0.0009665871621109545, 0.0009677475318312645, 0.0009748410666361451, 0.9898834824562073, 0.000952236179728061, 0.0010369858937337995, 0.0011633470421656966, 0.0009643258526921272, 0.00108472746796906, 0.0009565422078594565, 0.0010492049623280764]'\n",
      "[[1.82839483e-03 9.89306509e-01 8.68268427e-04 8.48613796e-04\n",
      "  8.48134805e-04 8.49004777e-04 8.54117796e-04 1.36881636e-03\n",
      "  8.49858625e-04 8.51970108e-04 1.52635586e-03]\n",
      " [9.59322439e-04 9.36145778e-04 9.49217647e-04 9.15768207e-04\n",
      "  9.15250450e-04 9.96708288e-04 9.21707775e-04 9.57637734e-04\n",
      "  9.90539074e-01 9.19390062e-04 9.89780296e-04]\n",
      " [8.75064638e-04 8.76114704e-04 8.82537395e-04 8.62559711e-04\n",
      "  8.62072047e-04 8.62956338e-04 8.68154224e-04 8.73016950e-04\n",
      "  8.63824564e-04 8.65971204e-04 9.91307795e-01]\n",
      " [9.66587162e-04 9.67747532e-04 9.74841067e-04 9.89883482e-01\n",
      "  9.52236180e-04 1.03698589e-03 1.16334704e-03 9.64325853e-04\n",
      "  1.08472747e-03 9.56542208e-04 1.04920496e-03]]\n",
      "(4, 11)\n",
      "(74960, 10)\n",
      "(10709, 10)\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "\n",
    "td = test_data.to_numpy()\n",
    "print(td[0, 1:])\n",
    "pred = xgb_predictor.predict(td[:4, 1:])\n",
    "print(\"pred = \", pred)\n",
    "raw_pred = pred[1:-1].replace(b'],[', b', ')\n",
    "\n",
    "pred_all = np.fromstring(raw_pred, sep=',').reshape(k, 11)\n",
    "print(pred_all)\n",
    "print(pred_all.shape)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10. 10.  4. ... 10. 10.  0.] [10 10  4 ... 10 10  0]\n",
      "['marker201' 'marker202' 'marker203' 'marker205' 'marker206' 'marker209'\n",
      " 'marker210' 'marker212' 'marker213' 'marker214' 'oil']\n",
      "oil\n"
     ]
    }
   ],
   "source": [
    "ind = np.fromstring(pred, sep=',')\n",
    "from main_forest import load_training_set\n",
    "from geo_config import training_set_path\n",
    "\n",
    "int_ind = ind.astype(int)\n",
    "print(ind, int_ind)\n",
    "filenames, marker_names = load_training_set(training_set_path)\n",
    "marker_names = np.append(marker_names, 'oil')\n",
    "print(marker_names)\n",
    "print(marker_names[int_ind[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998132411989915\n",
      "[10. 10.  4. 10. 10. 10. 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.accuracy_score(test_data.to_numpy()[:, 0], pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2csv(arr):\n",
    "    csv = io.BytesIO()\n",
    "    np.savetxt(csv, arr, delimiter=',', fmt='%g')\n",
    "    return csv.getvalue().decode().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime= boto3.client('runtime.sagemaker')\n",
    "payload = np2csv(td[:, 1:])\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                   ContentType='text/csv',\n",
    "                                   Body=payload)\n",
    "print(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import PCA\n",
    "num_components=8\n",
    "\n",
    "pca_SM = PCA(role=role,\n",
    "             train_instance_count=1,\n",
    "             train_instance_type='ml.c4.xlarge',\n",
    "             output_path='s3://'+ bucket +'/pca/',\n",
    "             num_components=num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BL-530-H', 'BL-586-H', 'BL-615-H', 'RL-660-H', 'RL-695-H', 'VL-445-H', 'VL-530-H', 'YL-586-H', 'YL-615-H']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(74960, 9)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geo_config import features_to_train\n",
    "train_data2 = train_data.values.astype('float32')[:,1:]\n",
    "print(features_to_train)\n",
    "train_data2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_SM.fit(pca_SM.record_set(train_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name = pca_SM.latest_training_job.name\n",
    "model_key = \"pca/\" + job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "boto3.resource('s3').Bucket(bucket).download_file(model_key, 'model.tar.gz')\n",
    "os.system('tar -zxvf model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-165abc4cd630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpca_model_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_algo-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "pca_model_params = mx.ndarray.load('model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "  Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 68.7 MB 108 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.18.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.22.0)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2019.11.28)\n",
      "Installing collected packages: graphviz, mxnet\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "pca_model_params = mx.ndarray.load('model_algo-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.DataFrame(pca_model_params['s'].asnumpy())\n",
    "v=pd.DataFrame(pca_model_params['v'].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.985834\n",
       "dtype: float32"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.iloc[4:,:].apply(lambda x: x*x).sum()/s.apply(lambda x: x*x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
